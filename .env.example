API_URL="http://localhost:8001"

# Ollama
OLLAMA_URL="http://ollama:11434"
OLLAMA_MODEL="llama3.1:latest"

# ChromaDB
CHROMA_COLLECTION=agent_memory
CHROMA_HOST="chroma"
CHROMA_PORT=8000
CHROMA_DIR="/chroma"

# Streamlit
STREAMLIT_PORT=8501

#####################

# Data Lake
DATA_LAKE_PATH=/data/lake

# AWS (Optional)
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
AWS_DEFAULT_REGION=us-east-1

# Datadog (Optional)
DATADOG_API_KEY=your_api_key
DATADOG_APP_KEY=your_app_key

# Database connections (Optional)
POSTGRES_CONNECTION_STRING=postgresql://user:pass@host:5432/db
MONGO_CONNECTION_STRING=mongodb://user:pass@host:27017/db
```

**Complete Workflow:**
```
1. Incident Triggered
         ↓
2. Data Collection Agent activates
         ↓
3. Parallel data collection from:
   - Docker logs (ollama, chroma, agent containers)
   - System metrics (CPU, memory, disk, network)
   - Prometheus queries
   - AlertManager alerts
   - AWS CloudWatch (logs + metrics)
   - GCP Logging
   - Datadog/New Relic APM
   - Database slow queries
   - Email alerts
         ↓
4. All data stored in Data Lake
   Structure: /data/lake/
      ├── logs/
      ├── metrics/
      ├── alerts/
      ├── cloud/
      ├── apm/
      ├── databases/
      └── raw/
         ↓
5. Data Lake creates snapshot
         ↓
6. Diagnosis Agent receives consolidated data
         ↓
7. Other agents process in sequence
         ↓
8. Complete incident analysis